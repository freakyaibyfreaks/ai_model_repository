{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16878cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd238c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: ../data/bentham.hdf5\n",
      "output ../output/bentham/flor\n",
      "target ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# Adding parent's path\n",
    "sys.path.append('../../')\n",
    "\n",
    "# define parameters\n",
    "source = \"bentham\"\n",
    "arch = \"flor\"\n",
    "epochs = 20 #1000\n",
    "batch_size = 16\n",
    "\n",
    "# define paths\n",
    "source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n",
    "output_path = os.path.join(\"..\", \"output\", source, arch)\n",
    "target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# define input size, number max of chars per line and list of valid chars\n",
    "input_size = (1024, 128, 1)\n",
    "max_text_length = 128\n",
    "charset_base = string.printable[:95]\n",
    "\n",
    "print(\"source:\", source_path)\n",
    "print(\"output\", output_path)\n",
    "print(\"target\", target_path)\n",
    "print(\"charset:\", charset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4acf5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82305b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 8807\n",
      "Validation images: 1372\n",
      "Test images: 820\n"
     ]
    }
   ],
   "source": [
    "from utils.generator import DataGenerator\n",
    "\n",
    "dtgen = DataGenerator(source=source_path,\n",
    "                      batch_size=batch_size,\n",
    "                      charset=charset_base,\n",
    "                      max_text_length=max_text_length)\n",
    "\n",
    "print(f\"Train images: {dtgen.size['train']}\")\n",
    "print(f\"Validation images: {dtgen.size['valid']}\")\n",
    "print(f\"Test images: {dtgen.size['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab2e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1024, 128, 1)]    0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 64, 16)       160       \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 512, 64, 16)       16        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512, 64, 16)       112       \n",
      "_________________________________________________________________\n",
      "full_gated_conv2d (FullGated (None, 512, 64, 16)       4640      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 64, 32)       4640      \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 512, 64, 32)       32        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 64, 32)       224       \n",
      "_________________________________________________________________\n",
      "full_gated_conv2d_1 (FullGat (None, 512, 64, 32)       18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 16, 40)       10280     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 256, 16, 40)       40        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 16, 40)       280       \n",
      "_________________________________________________________________\n",
      "full_gated_conv2d_2 (FullGat (None, 256, 16, 40)       28880     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 16, 40)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 16, 48)       17328     \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 256, 16, 48)       48        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 16, 48)       336       \n",
      "_________________________________________________________________\n",
      "full_gated_conv2d_3 (FullGat (None, 256, 16, 48)       41568     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256, 16, 48)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 4, 56)        21560     \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 128, 4, 56)        56        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 4, 56)        392       \n",
      "_________________________________________________________________\n",
      "full_gated_conv2d_4 (FullGat (None, 128, 4, 56)        56560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 4, 56)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 4, 64)        32320     \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 128, 4, 64)        64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 4, 64)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 2, 64)        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 256)          198144    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 256)          65792     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 256)          296448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 98)           25186     \n",
      "=================================================================\n",
      "Total params: 824,050\n",
      "Trainable params: 822,770\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from src.model.model import HTRModel\n",
    "\n",
    "# create and compile HTRModel\n",
    "model = HTRModel(architecture=arch,\n",
    "                 input_size=input_size,\n",
    "                 vocab_size=dtgen.tokenizer.vocab_size,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15)\n",
    "\n",
    "model.compile(learning_rate=0.001)\n",
    "model.summary(output_path, \"summary.txt\")\n",
    "\n",
    "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
    "model.load_checkpoint(target=target_path)\n",
    "\n",
    "callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50cc928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "551/551 [==============================] - 1076s 2s/step - loss: 19.8185 - val_loss: 18.0454\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 18.04536, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 2/20\n",
      "551/551 [==============================] - 1053s 2s/step - loss: 20.2410 - val_loss: 17.0708\n",
      "\n",
      "Epoch 00002: val_loss improved from 18.04536 to 17.07082, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 3/20\n",
      "551/551 [==============================] - 1047s 2s/step - loss: 18.4558 - val_loss: 16.4712\n",
      "\n",
      "Epoch 00003: val_loss improved from 17.07082 to 16.47123, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 4/20\n",
      "551/551 [==============================] - 1044s 2s/step - loss: 17.6355 - val_loss: 14.9740\n",
      "\n",
      "Epoch 00004: val_loss improved from 16.47123 to 14.97396, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 5/20\n",
      "551/551 [==============================] - 1042s 2s/step - loss: 16.7543 - val_loss: 15.2277\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 14.97396\n",
      "Epoch 6/20\n",
      "551/551 [==============================] - 1042s 2s/step - loss: 16.1760 - val_loss: 15.1366\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 14.97396\n",
      "Epoch 7/20\n",
      "551/551 [==============================] - 1036s 2s/step - loss: 15.1734 - val_loss: 14.2944\n",
      "\n",
      "Epoch 00007: val_loss improved from 14.97396 to 14.29444, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 8/20\n",
      "551/551 [==============================] - 1027s 2s/step - loss: 15.4220 - val_loss: 13.5347\n",
      "\n",
      "Epoch 00008: val_loss improved from 14.29444 to 13.53467, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 9/20\n",
      "551/551 [==============================] - 1029s 2s/step - loss: 14.6540 - val_loss: 13.3649\n",
      "\n",
      "Epoch 00009: val_loss improved from 13.53467 to 13.36485, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 10/20\n",
      "551/551 [==============================] - 1021s 2s/step - loss: 14.2915 - val_loss: 12.9664\n",
      "\n",
      "Epoch 00010: val_loss improved from 13.36485 to 12.96643, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 11/20\n",
      "551/551 [==============================] - 1024s 2s/step - loss: 14.1115 - val_loss: 13.1378\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 12.96643\n",
      "Epoch 12/20\n",
      "551/551 [==============================] - 1022s 2s/step - loss: 13.9404 - val_loss: 12.4362\n",
      "\n",
      "Epoch 00012: val_loss improved from 12.96643 to 12.43617, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 13/20\n",
      "551/551 [==============================] - 1022s 2s/step - loss: 13.4182 - val_loss: 12.1035\n",
      "\n",
      "Epoch 00013: val_loss improved from 12.43617 to 12.10350, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 14/20\n",
      "551/551 [==============================] - 1019s 2s/step - loss: 12.9442 - val_loss: 12.0534\n",
      "\n",
      "Epoch 00014: val_loss improved from 12.10350 to 12.05337, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 15/20\n",
      "551/551 [==============================] - 1017s 2s/step - loss: 13.0095 - val_loss: 12.0965\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 12.05337\n",
      "Epoch 16/20\n",
      "551/551 [==============================] - 1012s 2s/step - loss: 12.2321 - val_loss: 11.6462\n",
      "\n",
      "Epoch 00016: val_loss improved from 12.05337 to 11.64620, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 17/20\n",
      "551/551 [==============================] - 1027s 2s/step - loss: 12.4260 - val_loss: 11.9409\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 11.64620\n",
      "Epoch 18/20\n",
      "551/551 [==============================] - 1029s 2s/step - loss: 12.3914 - val_loss: 12.1106\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 11.64620\n",
      "Epoch 19/20\n",
      "551/551 [==============================] - 1042s 2s/step - loss: 12.2687 - val_loss: 11.6185\n",
      "\n",
      "Epoch 00019: val_loss improved from 11.64620 to 11.61846, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Epoch 20/20\n",
      "551/551 [==============================] - 1038s 2s/step - loss: 11.7173 - val_loss: 11.6148\n",
      "\n",
      "Epoch 00020: val_loss improved from 11.61846 to 11.61478, saving model to ../output/bentham/flor/checkpoint_weights.hdf5\n",
      "Total train images:      8807\n",
      "Total validation images: 1372\n",
      "Batch:                   16\n",
      "\n",
      "Total time:              5:44:31.671099\n",
      "Time per epoch:          0:17:13.583555\n",
      "Time per item:           0:00:00.101541\n",
      "\n",
      "Total epochs:            20\n",
      "Best epoch               20\n",
      "\n",
      "Training loss:           11.77905846\n",
      "Validation loss:         11.61478138\n"
     ]
    }
   ],
   "source": [
    "# to calculate total and average time per epoch\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "h = model.fit(x=dtgen.next_train_batch(),\n",
    "              epochs=epochs,\n",
    "              steps_per_epoch=dtgen.steps['train'],\n",
    "              validation_data=dtgen.next_valid_batch(),\n",
    "              validation_steps=dtgen.steps['valid'],\n",
    "              callbacks=callbacks,\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "\n",
    "total_time = datetime.datetime.now() - start_time\n",
    "\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "min_val_loss = min(val_loss)\n",
    "min_val_loss_i = val_loss.index(min_val_loss)\n",
    "\n",
    "time_epoch = (total_time / len(loss))\n",
    "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
    "\n",
    "t_corpus = \"\\n\".join([\n",
    "    f\"Total train images:      {dtgen.size['train']}\",\n",
    "    f\"Total validation images: {dtgen.size['valid']}\",\n",
    "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
    "    f\"Total time:              {total_time}\",\n",
    "    f\"Time per epoch:          {time_epoch}\",\n",
    "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
    "    f\"Total epochs:            {len(loss)}\",\n",
    "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
    "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
    "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
    "    lg.write(t_corpus)\n",
    "    print(t_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c62ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for turing: "
     ]
    }
   ],
   "source": [
    "!sudo update-pciids Rishabh.1@\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc8736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
